{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fj2aswYpniVY"
   },
   "source": [
    "\n",
    "# GAN Project - SoAI - WK6 - Navneet Dubey\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is a Generative Adversarial Network (GAN) implementation using PyTorch, specifically designed to generate face images using the CelebA (Celebrity Faces) dataset. The goal is to train a neural network that can generate realistic-looking human face images that don't actually exist, demonstrating the power of generative machine learning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IkgOsnPeDTX9"
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XIlY-7zsDk_W"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2z1Dvq6DtcV"
   },
   "outputs": [],
   "source": [
    "!kaggle datasets download -d jessicali9530/celeba-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CdjrxTe5D00r"
   },
   "outputs": [],
   "source": [
    "!unzip celeba-dataset.zip -d data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ty0WzEUKoWIa"
   },
   "source": [
    "## Importing & Initializing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r2nQ3OM5DGi6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "image_size = 64\n",
    "nz = 100  # Latent vector size\n",
    "ngf = 64  # Generator feature map size\n",
    "ndf = 64  # Discriminator feature map size\n",
    "num_epochs = 5\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iSZcD0d5EPBt"
   },
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "dataset = dset.ImageFolder(root=\"data\",\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7jmi48XbojYp"
   },
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IUYKOVVQEPjQ"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdEWDpX-ohlK"
   },
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3OftvjeonMC"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(3, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSen4rcIpIRi"
   },
   "source": [
    "### Custom weights initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Uji-BBLEXwy"
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ux8nTeJEcIZ"
   },
   "outputs": [],
   "source": [
    "netG = Generator().to(device)\n",
    "netD = Discriminator().to(device)\n",
    "\n",
    "# Initialize weights\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fyc9N1oIpYKD"
   },
   "source": [
    "## Loss & Optimizatation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iKGlUO0rpcPJ"
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0W7eXt1phr5"
   },
   "source": [
    "## Traning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jra41S0TEfOB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create directory if it does not exist\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        # Train Discriminator\n",
    "        netD.zero_grad()\n",
    "        real_data = data[0].to(device)\n",
    "        b_size = real_data.size(0)\n",
    "        label = torch.full((b_size,), 1., device=device)\n",
    "        output = netD(real_data).view(-1)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        fake_data = netG(noise)\n",
    "        label.fill_(0.)\n",
    "        output = netD(fake_data.detach()).view(-1)\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        optimizerD.step()\n",
    "\n",
    "        # Train Generator\n",
    "        netG.zero_grad()\n",
    "        label.fill_(1.)\n",
    "        output = netD(fake_data).view(-1)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "\n",
    "        # Print stats and save images\n",
    "        if i % 100 == 0:\n",
    "            print(f'[{epoch}/{num_epochs}][{i}/{len(dataloader)}] '\n",
    "                  f'Loss_D: {errD_real.item() + errD_fake.item()} Loss_G: {errG.item()}')\n",
    "            vutils.save_image(real_data, f'results/real_samples.png', normalize=True)\n",
    "            fake = netG(torch.randn(b_size, nz, 1, 1, device=device))\n",
    "            vutils.save_image(fake.detach(), f'results/fake_samples_epoch_{epoch}.png', normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYHHkZOXEkLt"
   },
   "outputs": [],
   "source": [
    "torch.save(netG.state_dict(), 'generator.pth')\n",
    "torch.save(netD.state_dict(), 'discriminator.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Stv8K5DTQ-We"
   },
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ND6cRpQ4Q-0u"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Define parameters\n",
    "image_folder = 'results'  # Folder where interpolated images are saved\n",
    "video_name = 'gan_interpolation_video.mp4'\n",
    "frame_rate = 3  # Frames per second\n",
    "\n",
    "# Get image paths\n",
    "images = sorted(glob(os.path.join(image_folder, 'fake_samples_epoch_*.png')))\n",
    "\n",
    "# Load the first image to get dimensions\n",
    "frame = cv2.imread(images[0])\n",
    "height, width, layers = frame.shape\n",
    "\n",
    "# Initialize video writer\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for .mp4 format\n",
    "video = cv2.VideoWriter(video_name, fourcc, frame_rate, (width, height))\n",
    "\n",
    "# Add each image to the video\n",
    "for image in images:\n",
    "    video.write(cv2.imread(image))\n",
    "\n",
    "# Release the video writer\n",
    "video.release()\n",
    "\n",
    "print(f\"Video saved as {video_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
